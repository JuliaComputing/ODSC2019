{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Zygote: Linear Regression\n",
    "\n",
    "In this notebook, we will define Linear Regression in Zygote from scratch, showing how easy it is to take derivatives of custom code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: activating environment at `~/src/msr_talk/Project.toml`.\n",
      "└ @ Pkg.API /Users/sabae/tmp/julia-build/julia-release-1.2/usr/share/julia/stdlib/v1.2/Pkg/src/API.jl:564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[2K\u001b[?25h[1mFetching:\u001b[22m\u001b[39m [========================================>]  100.0 %.0 %"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /Users/sabae/.julia/compiled/v1.2/Zygote/4kbLI.ji for Zygote [e88e6eb3-aa80-5325-afca-941959d7151f]\n",
      "└ @ Base loading.jl:1240\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment in current directory, to load\n",
    "import Pkg; Pkg.activate(@__DIR__); Pkg.instantiate()\n",
    "using Zygote, LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will showcase how we do a simple linear fit with Zygote, making use of complex datastructures, a home-grown stochastic gradient descent optimizer, and some good old-fashioned math.  We start with the problem\n",
    "statement:  We wish to learn the mapping `f(X) -> Y`, where `X` is a matrix of vector observations, `f()` is a linear mapping function and `Y` is a vector of scalar observations.\n",
    "\n",
    "Because we like complex objects, we will define our linear regression as the following object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LinearRegression object, containing multiple fields, some of which will be learned.\n",
    "mutable struct LinearRegression\n",
    "    # These values will be implicitly learned\n",
    "    weights::Matrix\n",
    "    bias::Float64\n",
    "\n",
    "    # These values will not be learned\n",
    "    name::String\n",
    "end\n",
    "\n",
    "LinearRegression(nparams, name) = LinearRegression(randn(1, nparams), 0.0, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define two verbs to act upon a `LinearRegression` object; `predict()`, to perform the linear regression, and `loss()` to measure the $\\ell_2$ norm between a target and our current prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our linear regression looks very familiar; w*X + b\n",
    "function predict(model::LinearRegression, X)\n",
    "    return model.weights * X .+ model.bias\n",
    "end\n",
    "\n",
    "# Our \"loss\" that must be minimized is the l2 norm between our current\n",
    "# prediction and our ground-truth Y\n",
    "function loss(model::LinearRegression, X, Y)\n",
    "    return norm(predict(model, X) .- Y, 2)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×10000 Array{Float64,2}:\n",
       " -1.19203    1.91616     0.843466  …  -1.57857    0.246021  -0.597997\n",
       " -0.48234    1.32798    -0.899407      0.043473   1.44814   -0.89648 \n",
       "  1.55866    0.0718622   1.89751       0.994072   0.638415  -0.273722\n",
       " -0.558112  -1.68074     0.954623      0.442166  -0.187531  -0.92237 "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our \"ground truth\" values (that we will learn, to prove that this works)\n",
    "weights_gt = [1.0, 2.7, 0.3, 1.2]'\n",
    "bias_gt = 0.4\n",
    "\n",
    "# Generate a dataset of many observations\n",
    "X = randn(length(weights_gt), 10000)\n",
    "Y = weights_gt * X .+ bias_gt\n",
    "\n",
    "# Add a little bit of noise to `X` so that we do not have an exact solution,\n",
    "# but must instead do a least-squares fit:\n",
    "X .+= 0.001.*randn(size(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Base.RefValue{Any}((weights = [-1.192026114794742 -0.482340240894484 1.5586551748882012 -0.5581118308574825], bias = 1.0, name = nothing)),)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we begin our \"training loop\", where we take examples from `X`,\n",
    "# calculate loss with respect to the corresponding entry in `Y`, find the\n",
    "# gradient upon our model, update the model, and continue.  Before we jump\n",
    "# in, let's look at what `Zygote.gradient()` gives us:\n",
    "model = LinearRegression(size(X, 1), \"Example\")\n",
    "\n",
    "# Calculate gradient upon `model` for the first example in our training set\n",
    "grads = Zygote.gradient(model) do m\n",
    "    return loss(m, X[:,1], Y[1])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `grads` object is a Tuple containing one element per argument to `gradient()`, so we take the first one to get the gradient upon `model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base.RefValue{Any}((weights = [-1.192026114794742 -0.482340240894484 1.5586551748882012 -0.5581118308574825], bias = 1.0, name = nothing))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads = grads[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our LinearRegression object is mutable, the gradient holds a reference to it, which we peel via `grads[]`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(weights = [-1.192026114794742 -0.482340240894484 1.5586551748882012 -0.5581118308574825], bias = 1.0, name = nothing)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads = grads[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get a `NamedTuple` so we can now do things like `grads.weights`. Note that while `weights` and `bias` have gradients, `name` just naturally has a  gradient of `nothing`, because it was not involved in the calculation of the output loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×4 Array{Float64,2}:\n",
       " -1.19203  -0.48234  1.55866  -0.558112"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define an update rule that will allow us to modify the weights of our model according to the gradients, using the simplest gradient descent update rule.  We'll then run a training loop to update our weights with the loss from the training set, as we would expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sgd_update! (generic function with 2 methods)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's define \n",
    "function sgd_update!(model::LinearRegression, grads, η = 0.001)\n",
    "    model.weights .-= η .* grads.weights\n",
    "    model.bias -= η * grads.bias\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Running train loop for 10000 iterations\n",
      "└ @ Main In[13]:2\n"
     ]
    }
   ],
   "source": [
    "# Now let's do that for each example in our training set:\n",
    "@info(\"Running train loop for $(size(X,2)) iterations\")\n",
    "for idx in 1:size(X, 2)\n",
    "    grads = Zygote.gradient(m -> loss(m, X[:, idx], Y[idx]), model)[1][]\n",
    "    sgd_update!(model, grads)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×4 Adjoint{Float64,Array{Float64,1}}:\n",
       " 1.0  2.7  0.3  1.2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×4 Array{Float64,2}:\n",
       " 1.00142  2.70157  0.300252  1.20033"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3980000000000003"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bias"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.2.0-rc1",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
